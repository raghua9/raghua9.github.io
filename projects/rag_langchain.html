<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>LangChain Chatbot</title>
    <script src="https://cdn.tailwindcss.com"></script>
</head>
<body class="bg-gray-900 text-white">
    <div class="container mx-auto p-6">
        <h1 class="text-4xl font-bold text-primary mb-6">LangChain Chatbot</h1>
        
        <section class="mb-8">
            <h2 class="text-2xl font-semibold text-secondary">Overview</h2>
            <p class="mt-2 text-gray-300">This project implements a chatbot using LangChain, OpenAI’s GPT-4o-mini, and Chroma for efficient conversational AI with memory and retrieval-augmented generation (RAG).</p>
        </section>

        <section class="mb-8">
            <h2 class="text-2xl font-semibold text-secondary">Tools Used</h2>
            <ul class="list-disc list-inside mt-2 text-gray-300">
                <li>Python 3.11+</li>
                <li>LangChain</li>
                <li>OpenAI API</li>
                <li>Gradio (for UI)</li>
                <li>Chroma (Vector Database)</li>
                <li>Dotenv (for environment variables)</li>
            </ul>
        </section>

        <section class="mb-8">
            <h2 class="text-2xl font-semibold text-secondary">Step-by-Step Implementation</h2>
            
            <div class="mt-4 p-4 bg-gray-800 rounded-lg">
                <h3 class="text-xl font-bold text-primary">1. Import Required Libraries</h3>
                <pre class="bg-gray-700 p-3 rounded">import os
import glob
from dotenv import load_dotenv
import gradio as gr
from langchain.document_loaders import DirectoryLoader, TextLoader
from langchain.text_splitter import CharacterTextSplitter
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_chroma import Chroma
from langchain.memory import ConversationBufferMemory
from langchain.chains import ConversationalRetrievalChain</pre>
                <p class="mt-2 text-gray-300">Necessary libraries are imported, including OpenAI’s GPT model, LangChain, and Chroma for vector storage.</p>
            </div>
            
            <div class="mt-4 p-4 bg-gray-800 rounded-lg">
                <h3 class="text-xl font-bold text-primary">2. Load Environment Variables</h3>
                <pre class="bg-gray-700 p-3 rounded">load_dotenv()
os.environ['OPENAI_API_KEY'] = os.getenv('OPENAI_API_KEY', 'your-key-if-not-using-env')
MODEL = "gpt-4o-mini"
db_name = "vector_db"</pre>
                <p class="mt-2 text-gray-300">Loads API keys securely using dotenv and sets up the model and database name.</p>
            </div>
            
            <div class="mt-4 p-4 bg-gray-800 rounded-lg">
                <h3 class="text-xl font-bold text-primary">3. Load and Process Documents</h3>
                <pre class="bg-gray-700 p-3 rounded">folders = glob.glob("knowledge-base/*")
text_loader_kwargs = {'encoding': 'utf-8'}

documents = []
for folder in folders:
    doc_type = os.path.basename(folder)
    loader = DirectoryLoader(folder, glob="**/*.md", loader_cls=TextLoader, loader_kwargs=text_loader_kwargs)
    folder_docs = loader.load()
    for doc in folder_docs:
        doc.metadata["doc_type"] = doc_type
        documents.append(doc)</pre>
                <p class="mt-2 text-gray-300">Reads and processes text documents for knowledge-based responses while retaining document type metadata.</p>
            </div>

            <div class="mt-4 p-4 bg-gray-800 rounded-lg">
                <h3 class="text-xl font-bold text-primary">4. Split Documents into Chunks</h3>
                <pre class="bg-gray-700 p-3 rounded">text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
chunks = text_splitter.split_documents(documents)</pre>
                <p class="mt-2 text-gray-300">Breaks documents into smaller chunks to improve retrieval efficiency.</p>
            </div>

            <div class="mt-4 p-4 bg-gray-800 rounded-lg">
                <h3 class="text-xl font-bold text-primary">5. Create Vector Store with Chroma</h3>
                <pre class="bg-gray-700 p-3 rounded">embeddings = OpenAIEmbeddings()

if os.path.exists(db_name):
    Chroma(persist_directory=db_name, embedding_function=embeddings).delete_collection()

vectorstore = Chroma.from_documents(documents=chunks, embedding=embeddings, persist_directory=db_name)</pre>
                <p class="mt-2 text-gray-300">Embeds and stores documents in ChromaDB for retrieval.</p>
            </div>

            <div class="mt-4 p-4 bg-gray-800 rounded-lg">
                <h3 class="text-xl font-bold text-primary">6. Set Up Chatbot Memory and LLM</h3>
                <pre class="bg-gray-700 p-3 rounded">memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)
llm = ChatOpenAI(temperature=0.7, model_name=MODEL)
retriever = vectorstore.as_retriever()
conversation_chain = ConversationalRetrievalChain.from_llm(llm=llm, retriever=retriever, memory=memory)</pre>
                <p class="mt-2 text-gray-300">Configures conversation memory, connects to OpenAI’s GPT model, and initializes the chatbot.</p>
            </div>
            
            <div class="mt-4 p-4 bg-gray-800 rounded-lg">
                <h3 class="text-xl font-bold text-primary">7. Define Chat Function and UI</h3>
                <pre class="bg-gray-700 p-3 rounded">def chat(message, history):
    result = conversation_chain.invoke({"question": message})
    return result["answer"]

gr.ChatInterface(chat, type="messages").launch(inbrowser=True)</pre>
                <p class="mt-2 text-gray-300">Sets up the chatbot interface using Gradio.</p>
            </div>
        </section>
    </div>
</body>
</html>
